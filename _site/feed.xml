<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-29T15:35:56-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Jakob Thumm</title><subtitle>This website contains research papers, blog posts, and other resources related to my research topics. It is a great resource for anyone interested in learning more about the science of robotics and how robots interact with humans.</subtitle><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><entry><title type="html">Text2Interaction</title><link href="http://localhost:4000/personalized-ai/text2interaction/" rel="alternate" type="text/html" title="Text2Interaction" /><published>2025-10-29T00:00:00-07:00</published><updated>2025-10-29T00:00:00-07:00</updated><id>http://localhost:4000/personalized-ai/text2interaction</id><content type="html" xml:base="http://localhost:4000/personalized-ai/text2interaction/">&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/img/B1_JakobBaseline1.gif&quot; alt=&quot;Baseline&quot; /&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/assets/img/E2_JakobHandleUpwards2.gif&quot; alt=&quot;Text2Interaction&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Baseline&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Text2Interaction&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Different users have varying preferences. This naturally applies to robotics as well!
Adjusting robot behavior to human preferences can require intensive human feedback, which prevents a quick adaptation to new users and changing circumstances.&lt;/p&gt;

&lt;p&gt;To integrate new user preferences in a zero-shot manner, we propose &lt;a href=&quot;https://sites.google.com/view/text2interaction&quot;&gt;Text2Interaction&lt;/a&gt;.
We use foundation models to generate task plans, motion preferences as Python code, and parameters of a safe controller that align with the user preferences.
By maximizing the combined probability of task completion and user satisfaction, we can reliably find plans that fulfill both requirements. 
In our real-world user study, we found that 83% of users think that it integrates their preferences into the robot’s plan, and 94% prefer Text2Interaction over the baseline.&lt;/p&gt;

&lt;p&gt;Publications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CoRL 2024: Text2Interaction: Establishing Safe and Preferable Human-Robot Interaction [&lt;a href=&quot;https://sites.google.com/view/text2interaction&quot;&gt;Website&lt;/a&gt;, &lt;a href=&quot;https://openreview.net/forum?id=s0VNSnPeoA&quot;&gt;OpenReview&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2408.06105&quot;&gt;Arxiv&lt;/a&gt;, &lt;a href=&quot;https://github.com/JakobThumm/text2interaction&quot;&gt;Github&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/LNfVw9ZtAtI&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="personalized-ai" /><summary type="html">Baseline Text2Interaction</summary></entry><entry><title type="html">Provably Safe RL</title><link href="http://localhost:4000/safe-rl/provably-safe-rl/" rel="alternate" type="text/html" title="Provably Safe RL" /><published>2025-10-28T00:00:00-07:00</published><updated>2025-10-28T00:00:00-07:00</updated><id>http://localhost:4000/safe-rl/provably-safe-rl</id><content type="html" xml:base="http://localhost:4000/safe-rl/provably-safe-rl/">&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/provably_safe_rl_structure.png&quot; alt=&quot;image-center&quot; width=&quot;800&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have to guarantee the safety of AI agents in deployment as they can show dangerous behavior in unseen situations.
As reinforcement learning (RL) agents learn by exploring the world, they are especially susceptible to returning unsafe actions.
In this line of work, we investigate how to best incorporate formal safety guarantees in the training of RL agents.&lt;/p&gt;

&lt;p&gt;We catogerize provably safe RL by the way the actions are altered, namely action replacement, action projection, and action masking.
Throughout all tasks, incorporating provable safety guarantees in the RL training significantly improves the performance of the agent. 
We find that action replacement is an easy to apply and effective method for simple environments.
In more complex environments, our new continuous action masking approach performs the best.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/provably_safe_rl_results.png&quot; alt=&quot;image-center&quot; width=&quot;600&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/action_masking_results.png&quot; alt=&quot;image-center&quot; width=&quot;800&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Publications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;NeurIPS 2024: Excluding the irrelevant: Focusing reinforcement learning through continuous action masking [&lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2024/file/acf4a08f67724e9d2de34099f57a9c25-Paper-Conference.pdf&quot;&gt;OpenReview&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;TMLR 2023: Provably safe reinforcement learning: Conceptual analysis, survey, and benchmarking [&lt;a href=&quot;https://openreview.net/pdf?id=mcN0ezbnzO&quot;&gt;OpenReview&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="safe-rl" /><summary type="html"></summary></entry><entry><title type="html">Human-robot gym</title><link href="http://localhost:4000/safe-rl/human-robot-gym/" rel="alternate" type="text/html" title="Human-robot gym" /><published>2023-02-01T00:00:00-08:00</published><updated>2023-02-01T00:00:00-08:00</updated><id>http://localhost:4000/safe-rl/human-robot-gym</id><content type="html" xml:base="http://localhost:4000/safe-rl/human-robot-gym/">&lt;p&gt;The field of collaborative robotics is rapidly growing and has the potential to change the way we live our day-to-day lives. From easing the work of factory workers, construction workers, and people with disabilities, to improving household tasks, the possibilities are endless. However, in order to achieve this, robots must be able to work in collaboration with humans in a dynamic and flexible way.&lt;/p&gt;

&lt;p&gt;Reinforcement learning (RL) has been showing great promise in solving challenging manipulation tasks on robots, and recent advancements in safe RL have made it feasible to apply deep learning-based controllers in safety-critical environments. 
To promote the development of safe, assistive RL agents, we are introducing a new benchmark suite called &lt;a href=&quot;(https://github.com/TUMcps/human-robot-gym)&quot;&gt;human-robot gym&lt;/a&gt;.
This platform is designed to train and evaluate learning-based policies in environments where humans exhibit diverse and complex behaviors. 
The agents learn to complete various tasks of varying difficulty levels, including reaching, pick-and-place, robot-human handover, object inspection, and collaborative construction tasks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/hrgym.png&quot; alt=&quot;image-center&quot; width=&quot;600&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To guarantee a natural human behavior, we recorded a large set of custom human animations for each task using a high-precision motion capture system. Additionally, to jump-start the development of safe collaborative RL methods, human-robot gym comes pre-implemented with our &lt;a href=&quot;/safe-rl/safety-shield/&quot;&gt;safety-shield&lt;/a&gt;, which guarantees human safety using reachability analysis.&lt;/p&gt;

&lt;p&gt;Publications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ICRA 2024: Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot Collaboration [&lt;a href=&quot;https://ieeexplore.ieee.org/document/10610705&quot;&gt;IEEExplore&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/2310.06208&quot;&gt;Arxiv&lt;/a&gt;, &lt;a href=&quot;https://github.com/TUMcps/human-robot gym&quot;&gt;Github&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="safe-rl" /><summary type="html">The field of collaborative robotics is rapidly growing and has the potential to change the way we live our day-to-day lives. From easing the work of factory workers, construction workers, and people with disabilities, to improving household tasks, the possibilities are endless. However, in order to achieve this, robots must be able to work in collaboration with humans in a dynamic and flexible way.</summary></entry><entry><title type="html">Mercedes-Benz R&amp;amp;D North America</title><link href="http://localhost:4000/internships/mbrdna/" rel="alternate" type="text/html" title="Mercedes-Benz R&amp;amp;D North America" /><published>2023-02-01T00:00:00-08:00</published><updated>2023-02-01T00:00:00-08:00</updated><id>http://localhost:4000/internships/mbrdna</id><content type="html" xml:base="http://localhost:4000/internships/mbrdna/">&lt;p&gt;During my 6-month internship at &lt;a href=&quot;https://mbrdna.com/&quot;&gt;Mercedes Benz Research and Development North America&lt;/a&gt;, I had the opportunity to work with a talented team of experts in the autonomous driving department. As part of the Software in the Loop (SiL) team, I was responsible for testing the autonomous driving software in simulation, in various traffic scenarios from around the world.&lt;/p&gt;

&lt;p&gt;One of the exciting projects I was involved in was the switch of HD map providers, where I wrote an extensive map converter to ensure our simulation could work seamlessly with both types of maps. This map converter allowed us to continue testing the software in simulation before deploying it to the real vehicle, making the process more efficient and cost-effective.&lt;/p&gt;

&lt;p&gt;During my time at Mercedes, I was able to make a significant contribution to the team by increasing the number of simulated autonomous miles by a factor of 10. I learned a lot about working effectively in a large software project, including how to handle Git, work in Agile projects, and properly test and document code.&lt;/p&gt;

&lt;p&gt;In addition to my professional development, I also had the opportunity to explore California and make life-long friends. Overall, my internship at Mercedes Benz was a fantastic experience that taught me a great deal about autonomous driving and the importance of teamwork and collaboration in software development.&lt;/p&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="internships" /><summary type="html">During my 6-month internship at Mercedes Benz Research and Development North America, I had the opportunity to work with a talented team of experts in the autonomous driving department. As part of the Software in the Loop (SiL) team, I was responsible for testing the autonomous driving software in simulation, in various traffic scenarios from around the world.</summary></entry><entry><title type="html">Optical Belt Sorting</title><link href="http://localhost:4000/study-projects/optical-belt-sorting/" rel="alternate" type="text/html" title="Optical Belt Sorting" /><published>2023-02-01T00:00:00-08:00</published><updated>2023-02-01T00:00:00-08:00</updated><id>http://localhost:4000/study-projects/optical-belt-sorting</id><content type="html" xml:base="http://localhost:4000/study-projects/optical-belt-sorting/">&lt;p&gt;Optical sorters are a game-changer in various industrial sectors like civil engineering, recycling, and agriculture. Unlike other sorters, optical sorters can sort based on visual properties alone, meaning almost any material type can be sorted without damage from water or heat. The standard layout of an optical sorter includes a transport unit (usually a conveyor belt), a line scan camera, and a separation mechanism. The line scan camera detects and classifies the particles, and the separation mechanism then separates the unwanted particles with short bursts of high-pressure air.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/optical_belt_sorter.png&quot; alt=&quot;image-right&quot; width=&quot;430&quot; class=&quot;align-right&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But what if we told you that this setup can be improved? In our previous research, we replaced the line scan camera with an area scan camera and implemented a predictive tracking approach. By tracking the particles with a Kalman filter, we were able to derive motion information and make separation predictions based on linear motion models. However, we found that this approach had its limitations - manual fine-tuning was required, and it didn’t always work for particles with nonlinear motion behavior.&lt;/p&gt;

&lt;p&gt;Enter our latest development: a mixture of experts (ME) concept. This innovative solution takes the best of both worlds - it combines the accuracy of neural networks (NNs) and the interpretability of physical motion models. We present two new NN experts for precise particle-specific predictions - a multilayer perceptron (MLP) and a long short-term memory (LSTM) - which we trained specifically for each particle type. However, NNs can lack generalization in new scenarios. So, our ME approach uses a gating network to weigh the predictions of different experts and produce a weighted sum of their predictions. This approach is unique because it combines physical motion models with pre-trained NNs, leading to a high level of accuracy and generalization.&lt;/p&gt;

&lt;p&gt;Our ME approach overcomes the limitations of previous methods. It increases sorting accuracy for a given particle type, it combines motion models and NNs to achieve high accuracy, and it leads to high accuracy in new, previously untrained cases. With its combination of high accuracy, interpretability, and validity, our ME approach is the first of its kind and opens up new possibilities for optical sorting technology.
For more information, see our Transactions on Industrial Informatics journal article:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Mixture of Experts of Neural Networks and Kalman Filters for Optical Belt Sorting [&lt;a href=&quot;https://ieeexplore.ieee.org/document/9543590&quot;&gt;IEEExplore&lt;/a&gt;] [&lt;a href=&quot;https://isas.iar.kit.edu/pdf/TII21_Thumm.pdf&quot;&gt;PDF&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="study-projects" /><summary type="html">Optical sorters are a game-changer in various industrial sectors like civil engineering, recycling, and agriculture. Unlike other sorters, optical sorters can sort based on visual properties alone, meaning almost any material type can be sorted without damage from water or heat. The standard layout of an optical sorter includes a transport unit (usually a conveyor belt), a line scan camera, and a separation mechanism. The line scan camera detects and classifies the particles, and the separation mechanism then separates the unwanted particles with short bursts of high-pressure air.</summary></entry><entry><title type="html">Road Condition Monitoring</title><link href="http://localhost:4000/study-projects/road-condition-monitoring/" rel="alternate" type="text/html" title="Road Condition Monitoring" /><published>2023-02-01T00:00:00-08:00</published><updated>2023-02-01T00:00:00-08:00</updated><id>http://localhost:4000/study-projects/road-condition-monitoring</id><content type="html" xml:base="http://localhost:4000/study-projects/road-condition-monitoring/">&lt;p&gt;Road maintenance is a crucial aspect of ensuring safe and efficient transportation. In Germany alone, over 10 billion Euros were spent on road maintenance projects in 2015, as reported by the German Federal Statistical Office. Poor road conditions can affect a wide range of factors, from vehicle operating costs and fuel efficiency to driving comfort and, most importantly, road safety. In 2015, over 1200 accidents in Germany were related to road hazards.&lt;/p&gt;

&lt;p&gt;Inspecting roads regularly is crucial in reducing the risk of accidents, but in many cases, smaller communities lack the resources to do so effectively. This is where our innovative solution comes in: an automatic road damage detection and labelling system. Our system aims to improve road condition monitoring and reduce the manual and financial effort required for regular inspections.&lt;/p&gt;

&lt;p&gt;Our method is based on a low-cost measurement device, consisting of an inertial sensor and GPS sensor, which is placed near the center of gravity of the vehicle. Using a machine learning algorithm and statistics calculated from vibrations and dynamics, the system can classify road infrastructure features and estimate the condition. The system also has the ability to collect the required training data automatically, avoiding the need for a physical model and manual training for each vehicle.&lt;/p&gt;

&lt;p&gt;In addition to providing periodic monitoring of roads, our system can also be used to compare output from multiple vehicles, improving the prediction of road conditions and enabling trend recognition. With our proposed method, road safety and quality can be improved with comparatively little expense.&lt;/p&gt;

&lt;p&gt;I started on this project in my bachelor’s thesis and continued the work for another year. In that time, we published our findings in four peer-reviewed journals:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Learning from the crowd: Road infrastructure monitoring system [&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2095756417300168&quot;&gt;Open-Access&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Multiple vehicle fusion for a robust road condition estimation based on vehicle sensors and data mining [&lt;a href=&quot;https://www.tandfonline.com/doi/full/10.1080/23311916.2018.1449428&quot;&gt;Open-Access&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Characterization of Road Condition with Data Mining Based on Measured Kinematic Vehicle Parameters [&lt;a href=&quot;https://www.hindawi.com/journals/jat/2018/8647607/&quot;&gt;Open-Access&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;A novel approach to label road defects in video data: semi-automated video analysis [&lt;a href=&quot;https://sciendo.com/es/article/10.21307/ijssis-2020-007&quot;&gt;Open-Access&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="study-projects" /><summary type="html">Road maintenance is a crucial aspect of ensuring safe and efficient transportation. In Germany alone, over 10 billion Euros were spent on road maintenance projects in 2015, as reported by the German Federal Statistical Office. Poor road conditions can affect a wide range of factors, from vehicle operating costs and fuel efficiency to driving comfort and, most importantly, road safety. In 2015, over 1200 accidents in Germany were related to road hazards.</summary></entry><entry><title type="html">Reducing Safety Interventions in Provably Safe Reinforcement Learning</title><link href="http://localhost:4000/safe-rl/reducing-failsafe/" rel="alternate" type="text/html" title="Reducing Safety Interventions in Provably Safe Reinforcement Learning" /><published>2023-01-29T00:00:00-08:00</published><updated>2023-01-29T00:00:00-08:00</updated><id>http://localhost:4000/safe-rl/reducing-failsafe</id><content type="html" xml:base="http://localhost:4000/safe-rl/reducing-failsafe/">&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/dIvhyV5z8bM&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;In our previous work, we showed how a &lt;a href=&quot;/safe-rl/safety-shield/&quot;&gt;safety shield&lt;/a&gt; can ensure the safety of RL agents in general robotic tasks.
When trained in simulation, the RL agent learns to evade the human to reach its goal.
Since a safety intervention slows down the robot, the agent learns a behavior that will have little interventions on average.
When we tested this behavior on the real robot, even small intrusions into the robots workspace led to strong safety interventions.
This indicates that the agent struggles to learn a reactive behavior that prevents failsafe interventions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/main_idea.png&quot; alt=&quot;image-right&quot; width=&quot;500&quot; class=&quot;align-right&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To address this problem, we propose a two new methods to reduce the interventions of our safety shield, action replacement and projection.
At each RL step, we check if the selected action is likely to cause a safety intervention and alter the action if that is the case.
Both methods cannot fully prevent all safety interventions as this would result in a very restrictive robot behavior.
However, our experiments show that the action projection method reduces the number of interventions to almost zero in two different robotic domains.
The safety reduction does not come with any loss of reward performance.&lt;/p&gt;

&lt;p&gt;Our real-world evaluations on a six degree-of-freedom manipulator showed strong improvements in the behavior of the safe RL agent when using our proposed projection method, as you can see in the video on top.&lt;/p&gt;

&lt;p&gt;For more a detailed description of our proposed methods, see our pre-print:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;IROS 2023: Reducing Safety Interventions in Provably Safe Reinforcement Learning [&lt;a href=&quot;https://ieeexplore.ieee.org/document/10342464&quot;&gt;IEEExplore&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2303.03339&quot;&gt;Arxiv&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="safe-rl" /><summary type="html">In our previous work, we showed how a safety shield can ensure the safety of RL agents in general robotic tasks. When trained in simulation, the RL agent learns to evade the human to reach its goal. Since a safety intervention slows down the robot, the agent learns a behavior that will have little interventions on average. When we tested this behavior on the real robot, even small intrusions into the robots workspace led to strong safety interventions. This indicates that the agent struggles to learn a reactive behavior that prevents failsafe interventions.</summary></entry><entry><title type="html">Safety shield for human-robot collaboration</title><link href="http://localhost:4000/safe-rl/safety-shield/" rel="alternate" type="text/html" title="Safety shield for human-robot collaboration" /><published>2023-01-29T00:00:00-08:00</published><updated>2023-01-29T00:00:00-08:00</updated><id>http://localhost:4000/safe-rl/safety-shield</id><content type="html" xml:base="http://localhost:4000/safe-rl/safety-shield/">&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/IUAeZGau28E&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;p&gt;We cannot directly deploy embodied AI agents in human environments, as they may exhibit unforeseen and erratic behavior under OOD conditions.
We solve this problem with our SARA shield that provides formal safety verification for autonomous robots in human environments.&lt;/p&gt;

&lt;p&gt;Our safety shield uses set-based reachability analysis to verify if a contact between the human and the robot is possible before we could stop the robot.
SARA shield comes in two modes: in the speed and separation monitoring (SSM), it guarantees that the robot is stopped before a collision could occur, and in the power and force limiting (PFL) mode, we guarantee pain-free contacts by limiting the kinetic energy of the robot before contact.
Our safety shield is the first of its kind, providing provable safety for continuous action spaces in high-dimensional state-spaces and unpredictable dynamic environments. It can be applied to a variety of manipulation tasks and is highly effective in quickly reacting to highly dynamic human motion.&lt;/p&gt;

&lt;p&gt;Publications:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;T-RO 2025: A General Safety Framework for Autonomous Manipulation in Human Environments [&lt;a href=&quot;https://arxiv.org/abs/2412.10180&quot;&gt;Arxiv&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;ICRA 2022: Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments [&lt;a href=&quot;https://ieeexplore.ieee.org/document/9811698&quot;&gt;IEEExplore&lt;/a&gt;] [&lt;a href=&quot;https://arxiv.org/abs/2205.06311&quot;&gt;Arxiv&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;ICRA 2022: SaRA: A Tool for Safe Human-Robot Coexistence and Collaboration through Reachability Analysis [&lt;a href=&quot;https://ieeexplore.ieee.org/document/9811952&quot;&gt;IEEExplore&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/h6Sn1aZDQe4&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/ldvWMJVBLa0&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;

&lt;!-- Courtesy of embedresponsively.com --&gt;

&lt;div class=&quot;responsive-video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube-nocookie.com/embed/p9jfqhiyxPY&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
  &lt;/div&gt;</content><author><name>Jakob Thumm</name><email>thumm@stanford.edu</email></author><category term="safe-rl" /><summary type="html">We cannot directly deploy embodied AI agents in human environments, as they may exhibit unforeseen and erratic behavior under OOD conditions. We solve this problem with our SARA shield that provides formal safety verification for autonomous robots in human environments.</summary></entry></feed>